# -*- coding: utf-8 -*-
"""2015312904_KwonJoonWoo_TFIDF_raw.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xFSAenMnXTc9zTyIP96nYB6IYgU8BVmW
"""



import nltk
import json
import math
from nltk.tokenize import word_tokenize
from google.colab import files

# Settings 1
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

path = './bbc_articles.json'
with open(path,'r') as f:
  json_data = json.load(f)

keylist = json_data.keys()
newpath = './2015312904_KwonJoonWoo_TFIDF_raw.txt'
f = open(newpath,'w')

# Setting Parameter_1
sample_list=['VB','VBD','VBG','VBN','VBP','VBZ','NN','NNS','NNP','NNPS']
Total_words = []
# ----------------------------------------------------------------------------
# Word list for the entire data 
for i in keylist:
  for temp in json_data[i]:
    tokens = word_tokenize(temp)
    tagged_tokens = nltk.pos_tag(tokens)

    # 한 article 안에서 word 중복 빼고 정렬
    for (word,pos) in tagged_tokens:
      new_word = word.lower()
      renew_word = new_word+'/'+pos
      if renew_word not in Total_words:
        if pos in sample_list:
          Total_words.append(renew_word)
Total_words.sort()
# ----------------------------------------------------------------------------
# Setting Parameter_2
temp_list = []
TF_list = [0]*len(Total_words)
IDF_list = []

# IDF 구하기 - 1
for i in keylist:
  for temp in json_data[i]:
    tokens = word_tokenize(temp)
    tagged_tokens = nltk.pos_tag(tokens)

    # 한 article 안에서 중복 없이 나열
    for (word,pos) in tagged_tokens:
      new_word = word.lower()
      renew_word = new_word+'/'+pos
      if renew_word not in temp_list:
        if pos in sample_list:
          temp_list.append(renew_word)
    temp_list.sort()
    
    # dft구하기 : 해당 article에서 나왔는지 안나왔는지
    num_2 = 0
    for k in Total_words:
      TF = TF_list[num_2]
      if k in temp_list:
        TF_list[num_2] = TF + 1
      else:
        TF_list[num_2] = TF
      num_2 += 1

    # Initialization
    del temp_list[:]

# IDF 구하기 - 2
for k in TF_list:
  N = 300 # Total number of document
  IDF = math.log(N/k)
  IDF_list.append(IDF)

# -----------------------------------------------------------------------------
# TF-IDF 구하기
list_TF =[]
TF_IDF = []

for i in keylist:
  index_num=1
  for temp in json_data[i]:
    tokens = word_tokenize(temp)
    tagged_tokens = nltk.pos_tag(tokens)
    new_num = str(index_num)
    f.write('('+i+','+new_num+')'+'\n')
    index_num += 1

    # article마다 데이터 다 모으기 
    for (word,pos) in tagged_tokens:
      new_word = word.lower()
      renew_word = new_word+'/'+pos
      temp_list.append(renew_word)

    # article의 TF 구하기
    for k in Total_words:
      TF = 0
      TF = temp_list.count(k)
      list_TF.append(TF)

    # article의 TF-IDF 구하기
    num_3 = 0
    for k in IDF_list:
      TF = list_TF[num_3]
      value = TF * k
      TF_IDF.append(value)
      num_3 += 1
    
    # Normalization
    def nor(list_1):
      # 제곱
      new_list = [k**2 for k in list_1]
      # 제곱 Sum
      sum = 0
      for k in new_list:
        sum += k
      # Root
      norm = abs(sum**0.5)
      # Normalize
      real_new_list = [k/norm for k in list_1]
      return real_new_list

    final_TFIDF = nor(TF_IDF)
    
    # Display
    for k in final_TFIDF:
      if k == 0:
        f.write('{}\t'.format(k))
      else:
        k = format(k,'.4f')
        f.write('{}\t'.format(k))
    
    # Initialization
    del temp_list[:]
    del list_TF[:]
    del TF_IDF[:]
    del final_TFIDF[:]
    f.write('\n')
    f.write('\n')
f.close()
from google.colab import drive
drive.mount('/content/drive')



